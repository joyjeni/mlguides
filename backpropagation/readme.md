#### 1. Gradient Descent
* The base algorithm that is used to minimize the error with respect to the weights of the neural network. The learning rate determines the step size of the update used to reach the minimum.
* An epoch is one complete pass through all the samples.
* The slope is called gradient
* -(-)=+ Weight increases (Moving Right)

### Negative Slope
When we increase w, the loss function is decreasing. 
### Positive Slope:
* When we increase w, the loss function is increasing.
* -(+) = - Weight Decreases. (Moving Left)


We move in the direction opposite to the derivative. (Opposite to the slope)



